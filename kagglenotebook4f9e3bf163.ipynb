{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T21:33:25.946800Z",
     "iopub.status.busy": "2023-08-09T21:33:25.946399Z",
     "iopub.status.idle": "2023-08-09T21:33:25.955320Z",
     "shell.execute_reply": "2023-08-09T21:33:25.953759Z",
     "shell.execute_reply.started": "2023-08-09T21:33:25.946770Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn \n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "from sklearn.linear_model._base import LinearModel, LinearClassifierMixin\n",
    "import numpy as np \n",
    "from torch.nn.modules.loss import _Loss\n",
    "from timeit import default_timer as timer\n",
    "import torch.nn.functional as F\n",
    "from scipy import optimize\n",
    "import copy\n",
    "\n",
    "\n",
    "outpath = '/kaggle/working/'\n",
    "data_path = '/kaggle/working/data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T21:33:26.598922Z",
     "iopub.status.busy": "2023-08-09T21:33:26.598355Z",
     "iopub.status.idle": "2023-08-09T21:33:26.972294Z",
     "shell.execute_reply": "2023-08-09T21:33:26.971008Z",
     "shell.execute_reply.started": "2023-08-09T21:33:26.598874Z"
    }
   },
   "outputs": [],
   "source": [
    "# layers\n",
    "class Linear(nn.Linear, LinearModel, LinearClassifierMixin):\n",
    "    def __init__(self, in_features, out_features, alpha=0.0, fit_bias=True,\n",
    "                 penalty=\"l2\", maxiter=1000):\n",
    "        super(Linear, self).__init__(in_features, out_features, fit_bias)\n",
    "        self.alpha = alpha\n",
    "        self.fit_bias = fit_bias\n",
    "        self.penalty = penalty\n",
    "        self.maxiter = maxiter\n",
    "\n",
    "    def forward(self, input, scale_bias=1.0):\n",
    "        # out = super(Linear, self).forward(input)\n",
    "        out = F.linear(input, self.weight, scale_bias * self.bias)\n",
    "        return out\n",
    "\n",
    "    def fit(self, x, y, criterion=None):\n",
    "        # self.cuda()\n",
    "        use_cuda = self.weight.is_cuda\n",
    "        # print(use_cuda)\n",
    "        if criterion is None:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        # reduction = criterion.reduction\n",
    "        # criterion.reduction = 'sum'\n",
    "        if isinstance(x, np.ndarray) or isinstance(y, np.ndarray):\n",
    "            x = torch.from_numpy(x)\n",
    "            y = torch.from_numpy(y)\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        alpha = self.alpha * x.shape[1] / x.shape[0]\n",
    "        if self.bias is not None:\n",
    "            scale_bias = (x ** 2).mean(-1).sqrt().mean().item()\n",
    "            alpha *= scale_bias ** 2\n",
    "        self.real_alpha = alpha\n",
    "        self.scale_bias = scale_bias\n",
    "\n",
    "        def eval_loss(w):\n",
    "            w = w.reshape((self.out_features, -1))\n",
    "            if self.weight.grad is not None:\n",
    "                self.weight.grad = None\n",
    "            if self.bias is None:\n",
    "                self.weight.data.copy_(torch.from_numpy(w))\n",
    "            else:\n",
    "                if self.bias.grad is not None:\n",
    "                    self.bias.grad = None\n",
    "                self.weight.data.copy_(torch.from_numpy(w[:, :-1]))\n",
    "                self.bias.data.copy_(torch.from_numpy(w[:, -1]))\n",
    "            y_pred = self(x, scale_bias=scale_bias).squeeze_(-1)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            if alpha != 0.0:\n",
    "                if self.penalty == \"l2\":\n",
    "                    penalty = 0.5 * alpha * torch.norm(self.weight)**2\n",
    "                elif self.penalty == \"l1\":\n",
    "                    penalty = alpha * torch.norm(self.weight, p=1)\n",
    "                    penalty.backward()\n",
    "                loss = loss + penalty\n",
    "            return loss.item()\n",
    "\n",
    "        def eval_grad(w):\n",
    "            dw = self.weight.grad.data\n",
    "            if alpha != 0.0:\n",
    "                if self.penalty == \"l2\":\n",
    "                    dw.add_(alpha, self.weight.data)\n",
    "            if self.bias is not None:\n",
    "                db = self.bias.grad.data\n",
    "                dw = torch.cat((dw, db.view(-1, 1)), dim=1)\n",
    "            return dw.cpu().numpy().ravel().astype(\"float64\")\n",
    "\n",
    "        w_init = self.weight.data\n",
    "        if self.bias is not None:\n",
    "            w_init = torch.cat((w_init, 1./scale_bias * self.bias.data.view(-1, 1)), dim=1)\n",
    "        w_init = w_init.cpu().numpy().astype(\"float64\")\n",
    "\n",
    "        w = optimize.fmin_l_bfgs_b(\n",
    "            eval_loss, w_init, fprime=eval_grad, maxiter=self.maxiter, disp=0)\n",
    "        if isinstance(w, tuple):\n",
    "            w = w[0]\n",
    "\n",
    "        w = w.reshape((self.out_features, -1))\n",
    "        self.weight.grad.data.zero_()\n",
    "        if self.bias is None:\n",
    "            self.weight.data.copy_(torch.from_numpy(w))\n",
    "        else:\n",
    "            self.bias.grad.data.zero_()\n",
    "            self.weight.data.copy_(torch.from_numpy(w[:, :-1]))\n",
    "            self.bias.data.copy_(scale_bias * torch.from_numpy(w[:, -1]))\n",
    "\n",
    "    def decision_function(self, x):\n",
    "        x = torch.from_numpy(x)\n",
    "        if self.weight.is_cuda:\n",
    "            x = x.cuda()\n",
    "        return self(x).data.cpu().numpy()\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.decision_function(x), axis=1)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        return self._predict_proba_lr(x)\n",
    "\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.weight.data.cpu().numpy()\n",
    "\n",
    "    @property\n",
    "    def intercept_(self):\n",
    "        return self.bias.data.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class CKNLayer(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "        padding=\"SAME\", dilation=1, groups=1, subsampling=1, bias=False,\n",
    "        kernel_func=\"exp\", kernel_args=[0.5], kernel_args_trainable=False):\n",
    "        \"\"\"Define a CKN layer\n",
    "        Args:\n",
    "            kernel_args: an iterable object of paramters for kernel function\n",
    "        \"\"\"\n",
    "        if padding == \"SAME\":\n",
    "            padding = kernel_size // 2\n",
    "        else:\n",
    "            padding = 0\n",
    "        super(CKNLayer, self).__init__(in_channels, out_channels, kernel_size, \n",
    "        stride=1, padding=padding, dilation=dilation, groups=groups, bias=False)\n",
    "        self.normalize_()\n",
    "        self.subsampling = subsampling\n",
    "\n",
    "        self.patch_dim = self.in_channels * self.kernel_size[0] * self.kernel_size[1]\n",
    "        \n",
    "        self._need_lintrans_computed = True \n",
    "\n",
    "        self.kernel_args_trainable = kernel_args_trainable\n",
    "        self.kernel_func = kernel_func\n",
    "        if isinstance(kernel_args, (int, float)):\n",
    "            kernel_args = [kernel_args]\n",
    "        if kernel_func == \"exp\":\n",
    "            kernel_args = [1./kernel_arg ** 2 for kernel_arg in kernel_args]\n",
    "        self.kernel_args = kernel_args\n",
    "        if kernel_args_trainable:\n",
    "            self.kernel_args = nn.ParameterList(\n",
    "                [nn.Parameter(torch.Tensor([kernel_arg])) for kernel_arg in kernel_args])\n",
    "\n",
    "        kernel_func = self._get_kernel(kernel_func)\n",
    "        self.kappa = lambda x: kernel_func(x, *self.kernel_args)\n",
    "\n",
    "        self.register_buffer(\"ones\",\n",
    "            torch.ones(1, self.in_channels // self.groups, *self.kernel_size))\n",
    "        self.init_pooling_filter()\n",
    "\n",
    "        self.ckn_bias = None\n",
    "        if bias:\n",
    "            self.ckn_bias = nn.Parameter(\n",
    "                torch.zeros(1, self.in_channels // self.groups, *self.kernel_size))\n",
    "\n",
    "        self.register_buffer(\"lintrans\",\n",
    "            torch.Tensor(out_channels, out_channels))\n",
    "        \n",
    "    def _get_kernel(self, kernel_func):\n",
    "        def exp(x, alpha):\n",
    "            \"\"\"Element wise non-linearity\n",
    "            kernel_exp is defined as k(x)=exp(alpha * (x-1))\n",
    "            return:\n",
    "                same shape tensor as x\n",
    "            \"\"\"\n",
    "            return torch.exp(alpha*(x - 1.))\n",
    "\n",
    "        def poly(x, alpha=None):\n",
    "            return x.pow(2)\n",
    "        return exp if kernel_func == \"exp\" else poly\n",
    "\n",
    "    def _gaussian_filter_1d(self, size, sigma=None):\n",
    "        \"\"\"Create 1D Gaussian filter\n",
    "        \"\"\"\n",
    "        if size == 1:\n",
    "            return torch.ones(1)\n",
    "        if sigma is None:\n",
    "            sigma = (size - 1.) / (2.*math.sqrt(2))\n",
    "        m = (size - 1) / 2.\n",
    "        filt = torch.arange(-m, m+1)\n",
    "        filt = torch.exp(-filt.pow(2)/(2.*sigma*sigma))\n",
    "        return filt/torch.sum(filt)   \n",
    "\n",
    "    def init_pooling_filter(self):\n",
    "        size = 2 * self.subsampling + 1\n",
    "        pooling_filter = self._gaussian_filter_1d(size, self.subsampling/math.sqrt(2)).view(-1, 1)\n",
    "        pooling_filter = pooling_filter.mm(pooling_filter.t())\n",
    "        pooling_filter = pooling_filter.expand(self.out_channels, 1, size, size).clone()\n",
    "        self.register_buffer(\"pooling_filter\", pooling_filter)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super(CKNLayer, self).train(mode)\n",
    "        self._need_lintrans_computed = True \n",
    "\n",
    "    def _compute_lintrans(self):\n",
    "        \"\"\"Compute the linear transformation factor kappa(ZtZ)^(-1/2)\n",
    "        Returns:\n",
    "            lintrans: out_channels x out_channels\n",
    "        \"\"\"\n",
    "        if not self._need_lintrans_computed:\n",
    "            return self.lintrans\n",
    "        lintrans = self.weight.view(self.out_channels, -1)\n",
    "        lintrans = lintrans.mm(lintrans.t())\n",
    "        lintrans = self.kappa(lintrans)\n",
    "        lintrans = matrix_inverse_sqrt(lintrans)\n",
    "        if not self.training:\n",
    "            self._need_lintrans_computed = False \n",
    "            self.lintrans.data = lintrans.data \n",
    "\n",
    "        return lintrans\n",
    "\n",
    "    def _conv_layer(self, x_in):\n",
    "        \"\"\"Convolution layer\n",
    "        Compute x_out = ||x_in|| x kappa(Zt x_in/||x_in||)\n",
    "        Args:\n",
    "            x_in: batch_size x in_channels x H x W\n",
    "            self.filters: out_channels x in_channels x *kernel_size\n",
    "            x_out: batch_size x out_channels x (H - kernel_size + 1) x (W - kernel_size + 1)\n",
    "        \"\"\"\n",
    "        if self.ckn_bias is not None:\n",
    "            # compute || x - b ||\n",
    "            patch_norm_x = F.conv2d(x_in.pow(2), self.ones, bias=None,\n",
    "                                    stride=1, padding=self.padding,\n",
    "                                    dilation=self.dilation, \n",
    "                                    groups=self.groups)\n",
    "            patch_norm = patch_norm_x - 2 * F.conv2d(x_in, self.ckn_bias, bias=None,\n",
    "                stride=1, padding=self.padding, dilation=self.dilation, \n",
    "                groups=self.groups)\n",
    "            patch_norm = patch_norm + self.ckn_bias.pow(2).sum()\n",
    "            patch_norm = torch.sqrt(patch_norm.clamp(min=1e-6))\n",
    "\n",
    "            x_out = super(CKNLayer, self).forward(x_in)\n",
    "            bias = torch.sum(\n",
    "                (self.weight * self.ckn_bias).view(self.out_channels, -1), dim=-1)\n",
    "            bias = bias.view(1, self.out_channels, 1, 1)\n",
    "            x_out = x_out - bias\n",
    "            x_out = x_out / patch_norm.clamp(min=1e-6)\n",
    "            x_out = patch_norm * self.kappa(x_out)\n",
    "            return x_out\n",
    "\n",
    "        patch_norm = torch.sqrt(F.conv2d(x_in.pow(2), self.ones, bias=None,\n",
    "            stride=1, padding=self.padding, dilation=self.dilation, \n",
    "            groups=self.groups).clamp(min=1e-6))\n",
    "\n",
    "        x_out = super(CKNLayer, self).forward(x_in)\n",
    "        x_out = x_out / patch_norm.clamp(min=1e-6)\n",
    "        x_out = patch_norm * self.kappa(x_out)\n",
    "        return x_out\n",
    "\n",
    "    def _mult_layer(self, x_in, lintrans):\n",
    "        \"\"\"Multiplication layer\n",
    "        Compute x_out = kappa(ZtZ)^(-1/2) x x_in\n",
    "        Args:\n",
    "            x_in: batch_size x in_channels x H x W\n",
    "            lintrans: in_channels x in_channels\n",
    "            x_out: batch_size x in_channels x H x W\n",
    "        \"\"\"\n",
    "        batch_size, in_c, H, W = x_in.size()\n",
    "        x_out = torch.bmm(\n",
    "            lintrans.expand(batch_size, in_c, in_c).clone(), x_in.view(batch_size, in_c, -1))\n",
    "        return x_out.view(batch_size, in_c, H, W)\n",
    "\n",
    "    def _pool_layer(self, x_in):\n",
    "        \"\"\"Pooling layer\n",
    "        Compute I(z) = \\sum_{z'} phi(z') x exp(-\\beta_1 ||z'-z||_2^2)\n",
    "        Args:\n",
    "            x_in: batch_size x out_channels x H x W\n",
    "        \"\"\"\n",
    "        if self.subsampling <= 1:\n",
    "            return x_in\n",
    "        x_out = F.conv2d(x_in, self.pooling_filter, bias=None, \n",
    "            stride=self.subsampling, padding=self.subsampling, \n",
    "            groups=self.out_channels)\n",
    "        return x_out\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"Encode function for a CKN layer\n",
    "        Args:\n",
    "            x_in: batch_size x in_channels x H x W\n",
    "        \"\"\"\n",
    "        x_out = self._conv_layer(x_in)\n",
    "        #print(x_out.shape)\n",
    "        x_out = self._pool_layer(x_out)\n",
    "        lintrans = self._compute_lintrans()\n",
    "        x_out = self._mult_layer(x_out, lintrans)\n",
    "        #print(x_out.shape)\n",
    "        return x_out\n",
    "\n",
    "    def extract_2d_patches(self, x):\n",
    "        \"\"\"\n",
    "        x: batch_size x C x H x W\n",
    "        out: (batch_size * nH * nW) x (C * kernel_size)\n",
    "        \"\"\"\n",
    "        h, w = self.kernel_size\n",
    "#         print(h)\n",
    "#         print(w)\n",
    "#         print(self.patch_dim)\n",
    "        return x.unfold(2, h, 1).unfold(3, w, 1).transpose(1, 3).contiguous().view(-1, self.patch_dim)\n",
    "\n",
    "    def sample_patches(self, x_in, n_sampling_patches=1000):\n",
    "        \"\"\"Sample patches from the given Tensor\n",
    "        Args:\n",
    "            x_in (batch_size x in_channels x H x W)\n",
    "            n_sampling_patches (int): number of patches to sample\n",
    "        Returns:\n",
    "            patches: (batch_size x (H - filter_size + 1)) x (in_channels x filter_size)\n",
    "        \"\"\"\n",
    "#         print(x_in)\n",
    "        patches = self.extract_2d_patches(x_in)\n",
    "        \n",
    "        n_sampling_patches = min(patches.size(0), n_sampling_patches)\n",
    "        patches = patches[:n_sampling_patches]\n",
    "        return patches\n",
    "\n",
    "    def unsup_train_(self, patches):\n",
    "        \"\"\"Unsupervised training for a CKN layer\n",
    "        Args:\n",
    "            patches: n x (in_channels x *kernel_size)\n",
    "        Updates:\n",
    "            filters: out_channels x in_channels x *kernel_size\n",
    "        \"\"\"\n",
    "        if self.ckn_bias is not None:\n",
    "            print(\"estimating bias\")\n",
    "            m_patches = patches.mean(0)\n",
    "            self.ckn_bias.data.copy_(m_patches.view_as(self.ckn_bias.data))\n",
    "            patches -= m_patches\n",
    "        patches = normalize_(patches)\n",
    "        block_size = None if self.patch_dim < 1000 else 10 * self.patch_dim\n",
    "        weight = spherical_kmeans(patches, self.out_channels, block_size=block_size)\n",
    "        weight = weight.view_as(self.weight.data)\n",
    "        self.weight.data.copy_(weight)\n",
    "        self._need_lintrans_computed = True \n",
    "\n",
    "    def normalize_(self):\n",
    "        norm = self.weight.data.view(\n",
    "            self.out_channels, -1).norm(p=2, dim=-1).view(-1, 1, 1, 1)\n",
    "        self.weight.data.div_(norm.clamp_(min=1e-6))\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = super(CKNLayer, self).extra_repr()\n",
    "        s += ', subsampling={}'.format(self.subsampling)\n",
    "        s += ', kernel=({}, {})'.format(self.kernel_func, self.kernel_args)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T21:42:50.721817Z",
     "iopub.status.busy": "2023-08-09T21:42:50.721335Z",
     "iopub.status.idle": "2023-08-09T21:42:50.757142Z",
     "shell.execute_reply": "2023-08-09T21:42:50.755389Z",
     "shell.execute_reply.started": "2023-08-09T21:42:50.721782Z"
    }
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def create_dataset(train=True, dataugmentation=False):\n",
    "    # load dataset\n",
    "        \n",
    "    tr = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.convert('RGB')),\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Lambda(rep),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    if dataugmentation:\n",
    "        dt = [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()]\n",
    "        tr = dt + tr\n",
    "    dataset = torchvision.datasets.MNIST(\n",
    "        data_path,\n",
    "        train=train,\n",
    "        transform=tr,\n",
    "        download=True,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def count_parameters(model):\n",
    "    count = 0\n",
    "    for param in model.parameters():\n",
    "        count += np.prod(param.data.size())\n",
    "    return count\n",
    "\n",
    "def normalize_(x, p=2, dim=-1):\n",
    "    norm = x.norm(p=p, dim=dim, keepdim=True)\n",
    "    x.div_(norm.clamp(min=1e-6))\n",
    "    return x \n",
    "\n",
    "def spherical_kmeans(x, n_clusters, max_iters=100, block_size=None, verbose=True, init=None):\n",
    "    \"\"\"Spherical kmeans\n",
    "    Args:\n",
    "        x (Tensor n_samples x n_features): data points\n",
    "        n_clusters (int): number of clusters\n",
    "    \"\"\"\n",
    "    print(x.shape)\n",
    "    use_cuda = x.is_cuda\n",
    "    n_samples, n_features = x.size()\n",
    "    if init is None:\n",
    "        indices = torch.randperm(n_samples)[:n_clusters]\n",
    "        if use_cuda:\n",
    "            indices = indices.cuda()\n",
    "        clusters = x[indices]\n",
    "\n",
    "    prev_sim = np.inf\n",
    "    tmp = x.new_empty(n_samples)\n",
    "    assign = x.new_empty(n_samples, dtype=torch.long)\n",
    "    if block_size is None or block_size == 0:\n",
    "        block_size = x.shape[0]\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # assign data points to clusters\n",
    "        for i in range(0, n_samples, block_size):\n",
    "            end_i = min(i + block_size, n_samples)\n",
    "            cos_sim = x[i: end_i].mm(clusters.t())\n",
    "            tmp[i: end_i], assign[i: end_i] = cos_sim.max(dim=-1)\n",
    "        # cos_sim = x.mm(clusters.t())\n",
    "        # tmp, assign = cos_sim.max(dim=-1)\n",
    "        sim = tmp.mean()\n",
    "        if (n_iter + 1) % 10 == 0 and verbose:\n",
    "            print(\"Spherical kmeans iter {}, objective value {}\".format(\n",
    "                n_iter + 1, sim))\n",
    "\n",
    "        # update clusters\n",
    "        for j in range(n_clusters):\n",
    "            index = assign == j\n",
    "            if index.sum().item() == 0:\n",
    "                idx = tmp.argmin()\n",
    "                clusters[j] = x[idx]\n",
    "                tmp[idx] = 1.\n",
    "            else:\n",
    "                xj = x[index]\n",
    "                c = xj.mean(0)\n",
    "                clusters[j] = c / c.norm().clamp(min=1e-6)\n",
    "\n",
    "        if torch.abs(prev_sim - sim)/(torch.abs(sim)+1e-20) < 1e-6:\n",
    "            break\n",
    "        prev_sim = sim\n",
    "    return clusters\n",
    "\n",
    "\n",
    "class MatrixInverseSqrt(torch.autograd.Function):\n",
    "    \"\"\"Matrix inverse square root for a symmetric definite positive matrix\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, eps=1e-2):\n",
    "        use_cuda = input.is_cuda\n",
    "        #if input.size(0) < 300:\n",
    "        #    input = input.cpu()\n",
    "        input = input.cpu()\n",
    "        #print(torch.isnan(input).any())\n",
    "        e, v = torch.linalg.eigh(input)\n",
    "        if use_cuda:\n",
    "            e = e.cuda()\n",
    "            v = v.cuda()\n",
    "        e.clamp_(min=0)\n",
    "        e_sqrt = e.sqrt_().add_(eps)\n",
    "        ctx.save_for_backward(e_sqrt, v)\n",
    "        e_rsqrt = e_sqrt.reciprocal()\n",
    "\n",
    "        output = v.mm(torch.diag(e_rsqrt).mm(v.t()))\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        e_sqrt, v = ctx.saved_variables\n",
    "        ei = e_sqrt.expand_as(v)\n",
    "        ej = e_sqrt.view([-1, 1]).expand_as(v)\n",
    "        f = torch.reciprocal((ei + ej) * ei * ej)\n",
    "        grad_input = -v.mm((f*(v.t().mm(grad_output.mm(v)))).mm(v.t()))\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "def matrix_inverse_sqrt(input, eps=1e-2):\n",
    "    \"\"\"Wrapper for MatrixInverseSqrt\"\"\"\n",
    "    return MatrixInverseSqrt.apply(input, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T21:42:51.082210Z",
     "iopub.status.busy": "2023-08-09T21:42:51.081774Z",
     "iopub.status.idle": "2023-08-09T21:42:51.133462Z",
     "shell.execute_reply": "2023-08-09T21:42:51.132419Z",
     "shell.execute_reply.started": "2023-08-09T21:42:51.082173Z"
    }
   },
   "outputs": [],
   "source": [
    "# models\n",
    "\n",
    "\n",
    "class CKNSequential(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels_list, kernel_sizes, \n",
    "                 subsamplings, kernel_funcs=None, kernel_args_list=None,\n",
    "                 kernel_args_trainable=False, **kwargs):\n",
    "\n",
    "        assert len(out_channels_list) == len(kernel_sizes) == len(subsamplings), \"incompatible dimensions\"\n",
    "        super(CKNSequential, self).__init__()\n",
    "\n",
    "        self.n_layers = len(out_channels_list)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels_list[-1]\n",
    "        \n",
    "        ckn_layers = []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            if kernel_funcs is None:\n",
    "                kernel_func = \"exp\"\n",
    "            else:\n",
    "                kernel_func = kernel_funcs[i] \n",
    "            if kernel_args_list is None:\n",
    "                kernel_args = 0.5\n",
    "            else:\n",
    "                kernel_args = kernel_args_list[i]\n",
    "            \n",
    "            ckn_layer = CKNLayer(in_channels, out_channels_list[i],\n",
    "                                 kernel_sizes[i], subsampling=subsamplings[i],\n",
    "                                 kernel_func=kernel_func, kernel_args=kernel_args,\n",
    "                                 kernel_args_trainable=kernel_args_trainable, **kwargs)\n",
    "\n",
    "            ckn_layers.append(ckn_layer)\n",
    "            in_channels = out_channels_list[i]\n",
    "\n",
    "        self.ckn_layers = nn.Sequential(*ckn_layers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ckn_layers[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ckn_layers)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.ckn_layers._modules.values().__iter__()\n",
    "\n",
    "    def forward_at(self, x, i=0):\n",
    "        assert x.size(1) == self.ckn_layers[i].in_channels, \"bad dimension\"\n",
    "        return self.ckn_layers[i](x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ckn_layers(x)\n",
    "\n",
    "    def representation(self, x, n=0):\n",
    "        if n == -1:\n",
    "            n = self.n_layers\n",
    "        for i in range(n):\n",
    "            x = self.forward_at(x, i)\n",
    "        return x \n",
    "\n",
    "    def normalize_(self):\n",
    "        for module in self.ckn_layers:\n",
    "            module.normalize_()\n",
    "\n",
    "    def unsup_train_(self, data_loader, n_sampling_patches=100000, use_cuda=False, top_layers=None):\n",
    "        \"\"\"\n",
    "        x: size x C x H x W \n",
    "        top_layers: module object represents layers before this layer\n",
    "        \"\"\"\n",
    "        self.train(False)\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        with torch.no_grad():\n",
    "            for i, ckn_layer in enumerate(self.ckn_layers):\n",
    "                print()\n",
    "                print('-------------------------------------')\n",
    "                print('   TRAINING LAYER {}'.format(i + 1))\n",
    "                print('-------------------------------------')\n",
    "                n_patches = 0 \n",
    "                try:\n",
    "                    n_patches_per_batch = (n_sampling_patches + len(data_loader) - 1) // len(data_loader) \n",
    "                except:\n",
    "                    n_patches_per_batch = 1000\n",
    "                patches = torch.Tensor(n_sampling_patches, ckn_layer.patch_dim)\n",
    "                if use_cuda:\n",
    "                    patches = patches.cuda()\n",
    "\n",
    "                for data, _ in data_loader:\n",
    "                    if use_cuda:\n",
    "                        data = data.cuda()\n",
    "                    # data = Variable(data, volatile=True)\n",
    "                    if top_layers is not None:\n",
    "                        data = top_layers(data)\n",
    "                    data = self.representation(data, i)\n",
    "#                     print(n_patches_per_batch)\n",
    "                    data_patches = ckn_layer.sample_patches(data.data, n_patches_per_batch)\n",
    "                    size = data_patches.size(0)\n",
    "                    if n_patches + size > n_sampling_patches:\n",
    "                        size = n_sampling_patches - n_patches\n",
    "                        data_patches = data_patches[:size]\n",
    "                    patches[n_patches: n_patches + size] = data_patches\n",
    "                    n_patches += size \n",
    "                    if n_patches >= n_sampling_patches:\n",
    "                        break\n",
    "\n",
    "                print(\"total number of patches: {}\".format(n_patches))\n",
    "                patches = patches[:n_patches]\n",
    "                ckn_layer.unsup_train_(patches)\n",
    "    \n",
    "class CKNet(nn.Module):\n",
    "    def __init__(self, nclass, in_channels, out_channels_list, kernel_sizes, \n",
    "                 subsamplings, kernel_funcs=None, kernel_args_list=None,\n",
    "                 kernel_args_trainable=False, image_size=32,\n",
    "                 fit_bias=True, alpha=0.0, maxiter=1000, **kwargs):\n",
    "        super(CKNet, self).__init__()\n",
    "        self.features = CKNSequential(\n",
    "            in_channels, out_channels_list, kernel_sizes, \n",
    "            subsamplings, kernel_funcs, kernel_args_list,\n",
    "            kernel_args_trainable, **kwargs)\n",
    "\n",
    "        out_features = out_channels_list[-1]\n",
    "        factor = 1\n",
    "        for s in subsamplings:\n",
    "            factor *= s\n",
    "        factor = (image_size - 1) // factor + 1\n",
    "        self.out_features = factor * factor * out_features\n",
    "        self.nclass = nclass\n",
    "\n",
    "        self.initialize_scaler()\n",
    "        self.classifier = Linear(\n",
    "            self.out_features, nclass, fit_bias=fit_bias, alpha=alpha, maxiter=maxiter)\n",
    "\n",
    "    def initialize_scaler(self, scaler=None):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        features = self.representation(input)\n",
    "        return self.classifier(features)\n",
    "\n",
    "    def representation(self, input):\n",
    "        features = self.features(input).view(input.shape[0], -1)\n",
    "        if hasattr(self, 'scaler'):\n",
    "            features = self.scaler(features)\n",
    "        return features\n",
    "\n",
    "    def unsup_train_ckn(self, data_loader, n_sampling_patches=1000000,\n",
    "                        use_cuda=False):\n",
    "        self.features.unsup_train_(data_loader, n_sampling_patches, use_cuda=use_cuda)\n",
    "\n",
    "    def unsup_train_classifier(self, data_loader, criterion=None, use_cuda=False):\n",
    "        encoded_train, encoded_target = self.predict(\n",
    "            data_loader, only_representation=True, use_cuda=use_cuda)\n",
    "        self.classifier.fit(encoded_train, encoded_target, criterion)\n",
    "\n",
    "    def predict(self, data_loader, only_representation=False, use_cuda=False):\n",
    "        self.eval()\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        n_samples = len(data_loader.dataset)\n",
    "        batch_start = 0\n",
    "        for i, (data, target) in enumerate(data_loader):\n",
    "            batch_size = data.shape[0]\n",
    "            if use_cuda:\n",
    "                data = data.cuda()\n",
    "            with torch.no_grad():\n",
    "                if only_representation:\n",
    "                    batch_out = self.representation(data).data.cpu()\n",
    "                else:\n",
    "                    batch_out = self(data).data.cpu()\n",
    "            if i == 0:\n",
    "                output = batch_out.new_empty(n_samples, batch_out.shape[-1])\n",
    "                target_output = target.new_empty(n_samples)\n",
    "            output[batch_start:batch_start+batch_size] = batch_out\n",
    "            target_output[batch_start:batch_start+batch_size] = target\n",
    "            batch_start += batch_size\n",
    "        return output, target_output\n",
    "\n",
    "    def normalize_(self):\n",
    "        self.features.normalize_()\n",
    "\n",
    "    def print_norm(self):\n",
    "        norms = []\n",
    "        with torch.no_grad():\n",
    "            for module in self.features:\n",
    "                norms.append(module.weight.sum().item())\n",
    "            norms.append(self.classifier.weight.sum().item())\n",
    "        print(norms)\n",
    "\n",
    "class SupCKNetMnist10_5(CKNet):\n",
    "    def __init__(self, alpha=0.0, **kwargs):\n",
    "        kernel_sizes = [3, 1, 3, 1, 3]\n",
    "        filters = [128, 128, 128, 128, 128]\n",
    "        subsamplings = [2, 1, 2, 1, 3]\n",
    "        kernel_funcs = ['exp', 'poly', 'exp', 'poly', 'exp']\n",
    "        kernel_args_list = [0.5, 2, 0.5, 2, 0.5]\n",
    "        super(SupCKNetMnist10_5, self).__init__(\n",
    "            10, 3, filters, kernel_sizes, subsamplings, kernel_funcs=kernel_funcs,\n",
    "            kernel_args_list=kernel_args_list, fit_bias=True, alpha=alpha, maxiter=5000, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T21:42:51.260091Z",
     "iopub.status.busy": "2023-08-09T21:42:51.258732Z",
     "iopub.status.idle": "2023-08-09T21:42:51.275321Z",
     "shell.execute_reply": "2023-08-09T21:42:51.273772Z",
     "shell.execute_reply.started": "2023-08-09T21:42:51.260033Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "class HingeLoss(_Loss):\n",
    "    def __init__(self, nclass=10, weight=None, size_average=None, reduce=None,\n",
    "                 reduction='elementwise_mean', pos_weight=None, squared=True):\n",
    "        super(HingeLoss, self).__init__(size_average, reduce, reduction)\n",
    "        self.nclass = nclass\n",
    "        self.squared = squared\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.register_buffer('pos_weight', pos_weight)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if not (target.size(0) == input.size(0)):\n",
    "            raise ValueError(\n",
    "                \"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
    "        if self.pos_weight is not None:\n",
    "            pos_weight = 1 + (self.pos_weight - 1) * target\n",
    "        target = 2 * F.one_hot(target, num_classes=self.nclass) - 1\n",
    "        target = target.float()\n",
    "        loss = F.relu(1. - target * input)\n",
    "        if self.squared:\n",
    "            loss = 0.5 * loss ** 2\n",
    "        if self.weight is not None:\n",
    "            loss = loss * self.weight\n",
    "        if self.pos_weight is not None:\n",
    "            loss = loss * pos_weight\n",
    "        loss = loss.sum(dim=-1)\n",
    "        if self.reduction == 'none':\n",
    "            return loss\n",
    "        elif self.reduction == 'elementwise_mean':\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T21:42:52.194149Z",
     "iopub.status.busy": "2023-08-09T21:42:52.193314Z",
     "iopub.status.idle": "2023-08-09T21:42:52.222526Z",
     "shell.execute_reply": "2023-08-09T21:42:52.220354Z",
     "shell.execute_reply.started": "2023-08-09T21:42:52.194094Z"
    }
   },
   "outputs": [],
   "source": [
    "# training\n",
    "def sup_train(model, data_loader):\n",
    "    criterion = HingeLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    \n",
    "    lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [60, 85, 100], gamma=0.1)\n",
    "       \n",
    "\n",
    "    print(\"Initialing CKN\")\n",
    "    tic = timer()\n",
    "    model.unsup_train_ckn(\n",
    "        data_loader['init'], 150000, use_cuda=False)\n",
    "    toc = timer()\n",
    "    print(\"Finished, elapsed time: {:.2f}min\".format((toc - tic)/60))\n",
    "\n",
    "    epoch_loss = None\n",
    "    best_loss = float('inf')\n",
    "    best_acc = 0\n",
    "#### change to 105\n",
    "    for epoch in range(15):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, 105))\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train(False)\n",
    "        tic = timer()\n",
    "        model.unsup_train_classifier(\n",
    "            data_loader['train'], criterion=criterion, use_cuda=False)\n",
    "        toc = timer()\n",
    "        print('Last layer trained, elapsed time: {:.2f}s'.format(toc - tic))\n",
    "            \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                if lr_scheduler is not None and epoch > 0:\n",
    "                    try:\n",
    "                        lr_scheduler.step(metrics=epoch_loss)\n",
    "                    except:\n",
    "                        lr_scheduler.step()\n",
    "                print(\"current LR: {}\".format(\n",
    "                            optimizer.param_groups[0]['lr']))\n",
    "                model.train()\n",
    "            else:\n",
    "                print(\"Evaluating...\")\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0\n",
    "\n",
    "            tic = timer()\n",
    "            for data, target in data_loader[phase]:\n",
    "                size = data.size(0)\n",
    "#                 if args.gpu:\n",
    "#                     data = data.cuda()\n",
    "#                     target = target.cuda()\n",
    "\n",
    "                # forward\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    pred = output.data.argmax(dim=1)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    model.normalize_()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        output = model(data)\n",
    "                        loss = criterion(output, target)\n",
    "                        pred = output.data.argmax(dim=1)\n",
    "                \n",
    "                running_loss += loss.item() * size\n",
    "                running_acc += torch.sum(pred == target.data).item()\n",
    "            toc = timer()\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = running_acc / len(data_loader[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.2f}% Elapsed time: {:.2f}s'.format(\n",
    "                    phase, epoch_loss, epoch_acc * 100, toc - tic))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "\n",
    "    print('Best epoch: {}'.format(best_epoch + 1))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "    model.load_state_dict(best_weights)\n",
    "\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T21:42:52.778646Z",
     "iopub.status.busy": "2023-08-09T21:42:52.778180Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "init_dset = create_dataset()\n",
    "train_dset = create_dataset(dataugmentation=False)\n",
    "test_dset = create_dataset(train=False)\n",
    "\n",
    "\n",
    "init_loader = DataLoader(\n",
    "        init_dset, batch_size=64, shuffle=False, num_workers=0)\n",
    "train_loader = DataLoader(\n",
    "        train_dset, batch_size=128, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(\n",
    "        test_dset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = SupCKNetMnist10_5(alpha=0.001)\n",
    "print(model)\n",
    "n_params = count_parameters(model)\n",
    "print('number of paramters: {}'.format(n_params))\n",
    "\n",
    "\n",
    "data_loader = {'init': init_loader, 'train': train_loader, 'val': test_loader}\n",
    "tic = timer()\n",
    "score = sup_train(model, data_loader)\n",
    "toc = timer()\n",
    "training_time = (toc - tic) / 60\n",
    "print(\"Final accuracy: {:6.2f}%, elapsed time: {:.2f}min\".format(score * 100, training_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "table = {'acc': score, 'training time': training_time}\n",
    "with open(outpath + '/metric.csv', 'w') as f:\n",
    "    w = csv.DictWriter(f, table.keys())\n",
    "    w.writeheader()\n",
    "    w.writerow(table)\n",
    "\n",
    "torch.save({\n",
    "    'state_dict': model.state_dict()},\n",
    "    outpath + '/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T21:34:00.951535Z",
     "iopub.status.busy": "2023-08-09T21:34:00.951070Z",
     "iopub.status.idle": "2023-08-09T21:34:01.346378Z",
     "shell.execute_reply": "2023-08-09T21:34:01.345328Z",
     "shell.execute_reply.started": "2023-08-09T21:34:00.951490Z"
    }
   },
   "outputs": [],
   "source": [
    "# init_dset = create_dataset()\n",
    "\n",
    "\n",
    "# init_loader = DataLoader(\n",
    "#         init_dset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # fig, axs = plt.subplots(nrows=int(np.ceil(np.sqrt(3))), ncols=int(np.ceil(np.sqrt(3))), figsize= (30,30))\n",
    "# # fig.suptitle('All reconstructed faces')\n",
    "# # axs = axs.flatten()\n",
    "# # print(test_loader.size())\n",
    "# for i, (data, target) in enumerate(init_loader):\n",
    "# #     print(target)\n",
    "# #     print(data)\n",
    "#     plt.imshow(data[0].T)\n",
    "#     break\n",
    "# #     print(data.size())\n",
    "# # #     face_plot = np.reshape(data, (200, 200)).T\n",
    "# #     ax.imshow(data[0], cmap='viridis')\n",
    "# #     ax.set_xticks([])\n",
    "# #     ax.set_yticks([])\n",
    "# #     ax.set_title(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_dset = create_dataset()\n",
    "# init_loader = DataLoader(\n",
    "#         init_dset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "# for data, _ in init_loader:\n",
    "#     print('x')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_patches_per_batch = (150 + len(init_loader) - 1) // len(init_loader) \n",
    "# n_patches_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(\n",
    "#         train_dset, batch_size=128, shuffle=True, num_workers=4)\n",
    "# len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dataset2( train=True, dataugmentation=False):\n",
    "#     # load dataset\n",
    "\n",
    "#     mean_pix = [x/255.0 for x in [125.3, 123.0, 113.9]]\n",
    "#     std_pix = [x/255.0 for x in [63.0, 62.1, 66.7]]\n",
    "#     tr = [transforms.ToTensor(), transforms.Normalize(mean=mean_pix, std=std_pix)]\n",
    "#     if dataugmentation:\n",
    "#         dt = [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip()]\n",
    "#         tr = dt + tr\n",
    "#     dataset = torchvision.datasets.CIFAR10(\n",
    "#         './data/',\n",
    "#         train=train,\n",
    "#         transform=transforms.Compose(tr),\n",
    "#         download=True,\n",
    "#     )\n",
    "#     return dataset\n",
    "    \n",
    "# init_dset2 = create_dataset2()\n",
    "\n",
    "# init_loader2 = DataLoader(\n",
    "#         init_dset, batch_size=64, shuffle=False, num_workers=2)\n",
    "# len(init_loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(init_loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data, _ in init_loader:\n",
    "#     print(data.shape)\n",
    "#     print(data)\n",
    "#     print(data.unfold(2, 3, 1).unfold(3, 3, 1).transpose(1, 3).contiguous().shape)\n",
    "\n",
    "\n",
    "#     y = data.unfold(2, 3, 1).unfold(3, 3, 1).transpose(1, 3).contiguous().view(-1, 27)\n",
    "\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data, _ in init_loader2:\n",
    "#     print(data.shape)\n",
    "#     print(data)\n",
    "    \n",
    "#     y = data.unfold(2, 3, 1).unfold(3, 3, 1).transpose(1, 3).contiguous().view(-1, 27)\n",
    "#     print(data.unfold(2, 3, 1).unfold(3, 3, 1).transpose(1, 3).contiguous().shape)\n",
    "#     print(y.shape)\n",
    "#     print(y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibiliography:\n",
    "https://github.com/Teo777Andrei/Image-Kernel<br>\n",
    "https://github.com/SaturdayGenfo/convolutionalKernelNet<br>\n",
    "https://github.com/ryanaleksander/kernel-convolution<br>\n",
    "https://github.com/cjones6/yesweckn<br>\n",
    "https://github.com/lemonhu/RE-CNN-pytorch<br>\n",
    "https://github.com/claying/CKN-Pytorch-image <br>\n",
    "https://github.com/YF-W/MU-Net<br>\n",
    "https://github.com/hsouri/DCNN_SVM<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
